{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing the waters\n",
    "\n",
    "Can ignore this section of code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/leannwoo/anaconda2/lib/python2.7/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.preprocessing.image import ImageDataGenerator, array_to_img, img_to_array, load_img\n",
    "import glob\n",
    "import cv2\n",
    "import numpy as np\n",
    "import PIL\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(units=64, activation='relu', input_dim=100))\n",
    "model.add(Dense(units=10, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='categorical_crossentropy', optimizer='sgd', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "datagen = ImageDataGenerator(rotation_range=40,\n",
    "                            width_shift_range=0.2,\n",
    "                            height_shift_range=0.2,\n",
    "                            rescale=1./255,\n",
    "                            shear_range=0.2,\n",
    "                            zoom_range=0.2,\n",
    "                            horizontal_flip=True,\n",
    "                            fill_mode='nearest')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = load_img('/Users/leannwoo/Dropbox/machine_learning_fishes/image_classifiers/Eyespots_all_species/eyespot/Apogon_Apogon cavitiensis_-1692469642.jpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = img_to_array(img)\n",
    "x = x.reshape((1,) + x.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 0\n",
    "for batch in datagen.flow(x, batch_size=1,\n",
    "                         save_to_dir='preview', save_prefix='eyespot', save_format='jpeg'):\n",
    "    i += 1\n",
    "    if i>20:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building our model\n",
    "\n",
    "Start paying attention here\n",
    "\n",
    "Solid explanation of kfold cross validation: https://www.openml.org/a/estimation-procedures/1  \n",
    "Code based on: https://www.kaggle.com/stefanie04736/simple-keras-model-with-k-fold-cross-validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Conv2D, MaxPooling2D\n",
    "from keras.layers import Activation, Dropout, Flatten, Dense\n",
    "from keras.utils import to_categorical\n",
    "from keras import backend as K\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
    "import numpy as np\n",
    "import glob\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "images = glob.glob('*.jpg')\n",
    "len(images)\n",
    "\n",
    "\n",
    "def getFish(filepath):\n",
    "    img = cv2.imread(filepath)\n",
    "    # standardize size for prediction step\n",
    "    img = cv2.resize(img, (150, 150))\n",
    "    img = img/255.0\n",
    "    img = img.reshape((1,) + img.shape)\n",
    "    \n",
    "    classification, _, species = filepath.partition('_')\n",
    "    \n",
    "    #res = model.predict(img)\n",
    "    return(img, classification, species)\n",
    "\n",
    "def load_data_kfold(k, image_filepaths):\n",
    "    \n",
    "    #use getFish function \n",
    "    #load in one file, append to a list\n",
    "    \n",
    "    x_train = []\n",
    "    y_train = []\n",
    "    \n",
    "    for i in range(len(image_filepaths)):\n",
    "        x, y, _ = getFish(image_filepaths[i])\n",
    "        x_train.append(x)\n",
    "        y_train.append(y)\n",
    "        if (y_train[i] == 'eyespot'):\n",
    "            y_train[i] = 0\n",
    "        else:\n",
    "            y_train[i] = 1\n",
    "    \n",
    "#     train = pd.read_json('../input/train.json')\n",
    "#     train.inc_angle = train.inc_angle.replace('na', 0)\n",
    "    \n",
    "#     x_band1 = np.array([np.array(band).astype(np.float32).reshape(75, 75) for band in train[\"band_1\"]])\n",
    "#     x_band2 = np.array([np.array(band).astype(np.float32).reshape(75, 75) for band in train[\"band_2\"]])\n",
    "#     x_band3 = x_band1 / x_band2\n",
    "       \n",
    "#     X_train = np.concatenate([x_band1[:, :, :, np.newaxis]\n",
    "#                             , x_band2[:, :, :, np.newaxis]\n",
    "#                             , x_band3[:, :, :, np.newaxis]], axis=-1)\n",
    "                         \n",
    "#     y_train = np.array(train[\"is_iceberg\"])\n",
    "    \n",
    "    #StratifiedKFold can only work in binary; convert labels to categorical afterwards\n",
    "    folds = list(StratifiedKFold(n_splits=k, shuffle=True, random_state=1).split(x_train, y_train))\n",
    "    \n",
    "    return folds, x_train, y_train\n",
    "\n",
    "k = 7\n",
    "folds, x_train, y_train = load_data_kfold(k, images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = to_categorical(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [1., 0.],\n",
       "       [0., 1.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [0., 1.],\n",
       "       [1., 0.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [0., 1.],\n",
       "       [1., 0.],\n",
       "       [0., 1.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [1., 0.],\n",
       "       [0., 1.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [1., 0.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [0., 1.],\n",
       "       [1., 0.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [1., 0.],\n",
       "       [0., 1.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [0., 1.],\n",
       "       [0., 1.]])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.shape\n",
    "y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/leannwoo/anaconda2/lib/python2.7/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# dimensions of our images.\n",
    "img_width, img_height = 150, 150\n",
    "\n",
    "train_data_dir = '/Users/leannwoo/Dropbox/machine_learning_fishes/image_classifiers/Eyespots_all_species/'\n",
    "validation_data_dir = '/Users/leannwoo/Dropbox/machine_learning_fishes/image_classifiers/Eyespots_all_species_validation/'\n",
    "nb_train_samples = 2000\n",
    "nb_validation_samples = 800\n",
    "epochs = 50\n",
    "batch_size = 16\n",
    "\n",
    "if K.image_data_format() == 'channels_first':\n",
    "    input_shape = (3, img_width, img_height)\n",
    "else:\n",
    "    input_shape = (img_width, img_height, 3)\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Conv2D(32, (3, 3), input_shape=input_shape))\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "model.add(Conv2D(32, (3, 3)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "model.add(Conv2D(64, (3, 3)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "model.add(Flatten())\n",
    "model.add(Dense(64))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(2))\n",
    "model.add(Activation('sigmoid'))\n",
    "\n",
    "model.compile(loss='mean_squared_error',\n",
    "              optimizer='rmsprop',\n",
    "              metrics=['accuracy'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 70 images belonging to 2 classes.\n",
      "Found 10 images belonging to 2 classes.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(150, 150, 3)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# this is the augmentation configuration we will use for training\n",
    "train_datagen = ImageDataGenerator(\n",
    "    rescale=1. / 255,\n",
    "    shear_range=0.2,\n",
    "    zoom_range=0.2,\n",
    "    horizontal_flip=True)\n",
    "\n",
    "# this is the augmentation configuration we will use for testing:\n",
    "# only rescaling\n",
    "test_datagen = ImageDataGenerator(rescale=1. / 255)\n",
    "\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "    train_data_dir,\n",
    "    target_size=(img_width, img_height),\n",
    "    batch_size=batch_size,\n",
    "    class_mode='categorical')\n",
    "\n",
    "validation_generator = test_datagen.flow_from_directory(\n",
    "    validation_data_dir,\n",
    "    target_size=(img_width, img_height),\n",
    "    batch_size=batch_size,\n",
    "    class_mode='categorical')\n",
    "\n",
    "train_generator[1][0][0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "125/125 [==============================] - 131s 1s/step - loss: 0.2319 - acc: 0.6078 - val_loss: 0.2845 - val_acc: 0.7000\n",
      "Epoch 2/50\n",
      "125/125 [==============================] - 117s 936ms/step - loss: 0.1324 - acc: 0.8282 - val_loss: 0.2761 - val_acc: 0.7000\n",
      "Epoch 3/50\n",
      "125/125 [==============================] - 113s 901ms/step - loss: 0.0622 - acc: 0.9237 - val_loss: 0.4330 - val_acc: 0.5000\n",
      "Epoch 4/50\n",
      "125/125 [==============================] - 111s 890ms/step - loss: 0.0305 - acc: 0.9687 - val_loss: 0.4811 - val_acc: 0.4000\n",
      "Epoch 5/50\n",
      "125/125 [==============================] - 112s 897ms/step - loss: 0.0217 - acc: 0.9722 - val_loss: 0.5457 - val_acc: 0.3000\n",
      "Epoch 6/50\n",
      "125/125 [==============================] - 113s 901ms/step - loss: 0.0153 - acc: 0.9845 - val_loss: 0.5676 - val_acc: 0.4000\n",
      "Epoch 7/50\n",
      "125/125 [==============================] - 109s 874ms/step - loss: 0.0110 - acc: 0.9872 - val_loss: 0.6038 - val_acc: 0.3000\n",
      "Epoch 8/50\n",
      "125/125 [==============================] - 108s 864ms/step - loss: 0.0076 - acc: 0.9907 - val_loss: 0.5076 - val_acc: 0.3000\n",
      "Epoch 9/50\n",
      "125/125 [==============================] - 116s 929ms/step - loss: 0.0091 - acc: 0.9905 - val_loss: 0.6464 - val_acc: 0.3000\n",
      "Epoch 10/50\n",
      "125/125 [==============================] - 107s 855ms/step - loss: 0.0101 - acc: 0.9917 - val_loss: 0.5249 - val_acc: 0.4000\n",
      "Epoch 11/50\n",
      "125/125 [==============================] - 107s 858ms/step - loss: 0.0050 - acc: 0.9962 - val_loss: 0.5581 - val_acc: 0.4000\n",
      "Epoch 12/50\n",
      "125/125 [==============================] - 109s 872ms/step - loss: 0.0115 - acc: 0.9847 - val_loss: 0.5938 - val_acc: 0.4000\n",
      "Epoch 13/50\n",
      "125/125 [==============================] - 108s 863ms/step - loss: 0.0060 - acc: 0.9945 - val_loss: 0.5642 - val_acc: 0.4000\n",
      "Epoch 14/50\n",
      "125/125 [==============================] - 108s 864ms/step - loss: 0.0148 - acc: 0.9865 - val_loss: 0.5065 - val_acc: 0.5000\n",
      "Epoch 15/50\n",
      "125/125 [==============================] - 107s 859ms/step - loss: 0.0040 - acc: 0.9962 - val_loss: 0.5739 - val_acc: 0.4000\n",
      "Epoch 16/50\n",
      "125/125 [==============================] - 107s 857ms/step - loss: 0.0088 - acc: 0.9902 - val_loss: 0.6134 - val_acc: 0.4000\n",
      "Epoch 17/50\n",
      "125/125 [==============================] - 109s 870ms/step - loss: 0.0081 - acc: 0.9903 - val_loss: 0.6217 - val_acc: 0.3000\n",
      "Epoch 18/50\n",
      "125/125 [==============================] - 106s 849ms/step - loss: 0.0064 - acc: 0.9923 - val_loss: 0.5408 - val_acc: 0.5000\n",
      "Epoch 19/50\n",
      "125/125 [==============================] - 105s 842ms/step - loss: 0.0060 - acc: 0.9945 - val_loss: 0.5727 - val_acc: 0.4000\n",
      "Epoch 20/50\n",
      "125/125 [==============================] - 105s 839ms/step - loss: 0.0030 - acc: 0.9975 - val_loss: 0.6999 - val_acc: 0.3000\n",
      "Epoch 21/50\n",
      "125/125 [==============================] - 105s 841ms/step - loss: 0.0102 - acc: 0.9887 - val_loss: 0.4305 - val_acc: 0.5000\n",
      "Epoch 22/50\n",
      "125/125 [==============================] - 105s 841ms/step - loss: 0.0103 - acc: 0.9880 - val_loss: 0.6987 - val_acc: 0.3000\n",
      "Epoch 23/50\n",
      "125/125 [==============================] - 106s 846ms/step - loss: 0.0062 - acc: 0.9935 - val_loss: 0.4999 - val_acc: 0.5000\n",
      "Epoch 24/50\n",
      "125/125 [==============================] - 106s 845ms/step - loss: 0.0026 - acc: 0.9965 - val_loss: 0.5665 - val_acc: 0.4000\n",
      "Epoch 25/50\n",
      "125/125 [==============================] - 109s 872ms/step - loss: 0.0056 - acc: 0.9945 - val_loss: 0.6313 - val_acc: 0.3000\n",
      "Epoch 26/50\n",
      "125/125 [==============================] - 111s 888ms/step - loss: 0.0102 - acc: 0.9887 - val_loss: 0.6978 - val_acc: 0.3000\n",
      "Epoch 27/50\n",
      "125/125 [==============================] - 128s 1s/step - loss: 0.0023 - acc: 0.9970 - val_loss: 0.5942 - val_acc: 0.4000\n",
      "Epoch 28/50\n",
      "125/125 [==============================] - 122s 974ms/step - loss: 0.0057 - acc: 0.9920 - val_loss: 0.5305 - val_acc: 0.4000\n",
      "Epoch 29/50\n",
      "125/125 [==============================] - 133s 1s/step - loss: 0.0022 - acc: 0.9970 - val_loss: 0.4410 - val_acc: 0.5000\n",
      "Epoch 30/50\n",
      "125/125 [==============================] - 126s 1s/step - loss: 0.0023 - acc: 0.9975 - val_loss: 0.6682 - val_acc: 0.3000\n",
      "Epoch 31/50\n",
      "125/125 [==============================] - 114s 916ms/step - loss: 0.0030 - acc: 0.9975 - val_loss: 0.5917 - val_acc: 0.4000\n",
      "Epoch 32/50\n",
      "125/125 [==============================] - 119s 951ms/step - loss: 0.0039 - acc: 0.9957 - val_loss: 0.6989 - val_acc: 0.3000\n",
      "Epoch 33/50\n",
      "125/125 [==============================] - 141s 1s/step - loss: 0.0049 - acc: 0.9938 - val_loss: 0.5817 - val_acc: 0.4000\n",
      "Epoch 34/50\n",
      "125/125 [==============================] - 131s 1s/step - loss: 0.0044 - acc: 0.9950 - val_loss: 0.6869 - val_acc: 0.3000\n",
      "Epoch 35/50\n",
      "125/125 [==============================] - 116s 927ms/step - loss: 0.0055 - acc: 0.9942 - val_loss: 0.6177 - val_acc: 0.3000\n",
      "Epoch 36/50\n",
      "125/125 [==============================] - 113s 906ms/step - loss: 0.0048 - acc: 0.9947 - val_loss: 0.6549 - val_acc: 0.3000\n",
      "Epoch 37/50\n",
      "125/125 [==============================] - 123s 980ms/step - loss: 0.0055 - acc: 0.9933 - val_loss: 0.6000 - val_acc: 0.4000\n",
      "Epoch 38/50\n",
      "125/125 [==============================] - 120s 957ms/step - loss: 0.0011 - acc: 0.9985 - val_loss: 0.6730 - val_acc: 0.3000\n",
      "Epoch 39/50\n",
      "125/125 [==============================] - 114s 910ms/step - loss: 0.0014 - acc: 0.9985 - val_loss: 0.6942 - val_acc: 0.3000\n",
      "Epoch 40/50\n",
      "125/125 [==============================] - 108s 865ms/step - loss: 0.0028 - acc: 0.9965 - val_loss: 0.4998 - val_acc: 0.5000\n",
      "Epoch 41/50\n",
      "125/125 [==============================] - 114s 910ms/step - loss: 0.0013 - acc: 0.9985 - val_loss: 0.4971 - val_acc: 0.5000\n",
      "Epoch 42/50\n",
      "125/125 [==============================] - 109s 875ms/step - loss: 0.0023 - acc: 0.9975 - val_loss: 0.4950 - val_acc: 0.5000\n",
      "Epoch 43/50\n",
      "125/125 [==============================] - 118s 942ms/step - loss: 0.0012 - acc: 0.9982 - val_loss: 0.5741 - val_acc: 0.4000\n",
      "Epoch 44/50\n",
      "125/125 [==============================] - 122s 975ms/step - loss: 0.0081 - acc: 0.9910 - val_loss: 0.5019 - val_acc: 0.5000\n",
      "Epoch 45/50\n",
      "125/125 [==============================] - 127s 1s/step - loss: 0.0056 - acc: 0.9937 - val_loss: 0.5959 - val_acc: 0.4000\n",
      "Epoch 46/50\n",
      "125/125 [==============================] - 119s 952ms/step - loss: 0.0019 - acc: 0.9975 - val_loss: 0.5000 - val_acc: 0.5000\n",
      "Epoch 47/50\n",
      "125/125 [==============================] - 125s 997ms/step - loss: 0.0061 - acc: 0.9933 - val_loss: 0.6720 - val_acc: 0.3000\n",
      "Epoch 48/50\n",
      "125/125 [==============================] - 122s 974ms/step - loss: 0.0056 - acc: 0.9942 - val_loss: 0.6008 - val_acc: 0.4000\n",
      "Epoch 49/50\n",
      "125/125 [==============================] - 116s 928ms/step - loss: 0.0015 - acc: 0.9985 - val_loss: 0.6000 - val_acc: 0.4000\n",
      "Epoch 50/50\n",
      "125/125 [==============================] - 117s 940ms/step - loss: 0.0022 - acc: 0.9980 - val_loss: 0.6994 - val_acc: 0.3000\n"
     ]
    }
   ],
   "source": [
    "model.fit_generator(\n",
    "    train_generator,\n",
    "    steps_per_epoch=nb_train_samples // batch_size,\n",
    "    epochs=epochs,\n",
    "    validation_data=validation_generator,\n",
    "    validation_steps=nb_validation_samples // batch_size)\n",
    "\n",
    "model.save_weights('first_try.h5')\n",
    "model.save('second_try.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reload model and use it to predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "\n",
    "def getFish(filepath):\n",
    "    img = cv2.imread(filepath)\n",
    "    # standardize size for prediction step\n",
    "    img = cv2.resize(img, (150, 150))\n",
    "    img = img/255.0\n",
    "    img = img.reshape((1,) + img.shape)\n",
    "    \n",
    "    classification, _, species = filepath.partition('_')\n",
    "    \n",
    "    #res = model.predict(img)\n",
    "    return(img, classification, species)\n",
    "\n",
    "#run through all images, classify,\n",
    "#and write out the classification category, \n",
    "#the confidence/weight of the prediction, \n",
    "#the species name, and the family. \n",
    "\n",
    "def predict(model, img):\n",
    "    prediction_weights = model.predict(img[0])\n",
    "    prediction = model.predict_classes(img[0])\n",
    "    if (prediction[0] == 0):\n",
    "        prediction = \"eyespot\"\n",
    "    else:\n",
    "        prediction = \"noeyespot\"\n",
    "    fam = img[2]\n",
    "    correct_class = img[1]\n",
    "    #print \"Model classification:\",prediction,\"\\nCorrect classification:\", correct_class,\"\\nPrediction wieghts:\",prediction_weights,\"\\nType:\",fam,\"\\n\"\n",
    "    return(prediction, correct_class, prediction_weights, fam)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('eyespot',\n",
       " 'eyespot',\n",
       " array([[1.000000e+00, 5.364594e-38]], dtype=float32),\n",
       " 'Abudefduf_Abudefduf sparoides_929157376.jpg')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras.models import load_model\n",
    "\n",
    "mod = load_model(\"second_try.h5\")\n",
    "\n",
    "eyepath = \"eyespot_Abudefduf_Abudefduf sparoides_929157376.jpg\"\n",
    "noeyepath = \"noeyespot_Abudefduf_Abudefduf bengalensis_880932306.jpg\"\n",
    "\n",
    "predict(mod, getFish(eyepath))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "80"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import glob\n",
    "\n",
    "images = glob.glob('*.jpg')\n",
    "len(images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "with open('classification_data.csv', 'wb') as csvfile:\n",
    "    filewriter = csv.writer(csvfile, delimiter=',',\n",
    "                            quotechar='|', quoting=csv.QUOTE_MINIMAL)\n",
    "    filewriter.writerow(['Model Classification', 'Correct Classification', 'Prediction Weights', 'Type'])\n",
    "\n",
    "    true_eye=0\n",
    "    true_noeye=0\n",
    "    false_eye=0\n",
    "    false_noeye=0\n",
    "\n",
    "    for i in range(len(images)):\n",
    "        info = predict(mod, getFish(images[i]))\n",
    "        filewriter.writerow([info[0], info[1], info[2], info[3]])\n",
    "\n",
    "        #computing info for confusion matrix\n",
    "        if (info[0] == info[1]):\n",
    "            if (info[0] == 'eyespot'):\n",
    "                true_eye += 1\n",
    "            else:\n",
    "                true_noeye += 1\n",
    "        else:\n",
    "            if (info[0] == 'eyespot'):\n",
    "                false_eye += 1\n",
    "            else:\n",
    "                false_noeye += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Eyespots</th>\n",
       "      <th>No Eyespots</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Eyespots (Model Output)</th>\n",
       "      <td>30</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>No Eyespots (Model Output)</th>\n",
       "      <td>10</td>\n",
       "      <td>36</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                            Eyespots  No Eyespots\n",
       "Eyespots (Model Output)           30            4\n",
       "No Eyespots (Model Output)        10           36"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "data = {'Eyespots':[true_eye, false_noeye], 'No Eyespots':[false_eye, true_noeye] }\n",
    "confusion_matrix = pd.DataFrame(data = data)\n",
    "confusion_matrix = confusion_matrix.rename({0:'Eyespots (Model Output)', 1:'No Eyespots (Model Output)'},axis='index')\n",
    "confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
